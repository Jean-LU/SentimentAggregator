# -*- coding: utf-8 -*-
"""TwitterSentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tbESrzmD7pZ0X5IHh3UwIwIpYs1wiK0V
"""

#Description: This script gives the sentiment of cryptocurrencies on Twitter

#Import the libraries
import tweepy
from textblob import TextBlob
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud
plt.style.use('fivethirtyeight')
import firebase_admin
from firebase_admin import credentials, firestore

#Upload the data
from google.colab import files
uploaded = files.upload()

cred = credentials.Certificate("credentials.json")
firebase_admin.initialize_app(cred)

#Upload the data
from google.colab import files
uploaded = files.upload()

#Get the data
log = pd.read_csv('TwitterKey.csv')

log.head()

#Twitter API credentials
consumerKey = log['Key'][0]
consumerSecret = log['Key'][1]
accessToken = log['Key'][2]
accessTokenSecret = log['Key'][3]

#Create the authentication object
authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret)

#Set the access token and access token secret
authenticate.set_access_token(accessToken, accessTokenSecret)

#Create the API object while passing in the auth information
api = tweepy.API(authenticate, wait_on_rate_limit = True)

#Make a dictionary of top 11 cryptos
CryptoDict = {}
CryptoDict["BTC"] = ['btc', 'bitcoin']
CryptoDict["ETH"] = ['eth', 'ethereum']
CryptoDict["ADA"] = ['ada', 'cardano']
CryptoDict["DOGE"] = ['doge', 'dogecoin']
CryptoDict["XRP"] = ['xrp', 'ripple']
CryptoDict["DOT"] = ['dot', 'polkadot']
CryptoDict["UNI"] = ['uni', 'uniswap']
CryptoDict["LTC"] = ['ltc', 'litecoin']
CryptoDict["SOL"] = ['SOL', 'solana']
CryptoDict["LINK"] = ['link', 'chainlink']
CryptoDict["MATIC"] = ['matic', 'poligon']

def scraper(CryptoDict, key):
  text_query = CryptoDict[key][0], CryptoDict[key][1]
  count = 100
  try:
 # Creation of query method using parameters
    tweets = tweepy.Cursor(api.search,q=text_query).items(count)
 
 # Pulling information from tweets iterable object
    tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]
 
 # Creation of dataframe from tweets list
    tweets_df = pd.DataFrame(tweets_list)

    tweets_df.columns = ['TimeStamp', 'TwitterID', 'Tweets']
 
  except BaseException as e:
    print('failed on_status,',str(e))
    time.sleep(3)
  
  return tweets_df

#Create a function to clean the tweets
def cleanTxt(text):
  text = re.sub(r'@[A-Za-z0-9]+', '', text) #Removes @mentions
  text = re.sub(r'#', '', text) #Rremoves the '#' symbol
  text = re.sub(r'RT[\s]+','', text) #Removes RT
  text = re.sub(r'https?:\/\/\s+', '', text) #Removes the hyper link

  return text

#Create a function to get the subjectivity
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

#Create a function to get the polarity
def getPolarity(text):
  return TextBlob(text).sentiment.polarity

def getAnalysis(score):
  if score < 0:
    return 'Negative'
  elif score == 0:
    return 'Neutral'
  else: 
    return 'Positive'

def sentiment_analysis(tweets_df, key):

  #Create two new columns
  tweets_df['Subjectivity'] = tweets_df['Tweets'].apply(getSubjectivity)
  tweets_df['Polarity'] = tweets_df['Tweets'].apply(getPolarity)

  tweets_df['Analysis'] = tweets_df['Polarity'].apply(getAnalysis)

    #Get the percentage of positive tweets
  ptweets = tweets_df[tweets_df.Analysis == 'Positive']
  ptweets = ptweets['Tweets']

  #Get the percentage of negative tweets
  ntweets = tweets_df[tweets_df.Analysis == 'Negative']
  ntweets = ntweets['Tweets']

  #Get the percentage of neutral tweets
  utweets = tweets_df[tweets_df.Analysis == 'Neutral']
  utweets = utweets['Tweets']

  json = {
    'ticker': "{}".format(key),
    'sentiment': {
        'twitter': {
            'positive': round((ptweets.shape[0] / tweets_df.shape[0])*100, 1),
            'neutral': round((utweets.shape[0] / tweets_df.shape[0])*100, 1),
            'negative': round((ntweets.shape[0] / tweets_df.shape[0])*100, 1),
          }
      }
  }

  return json

for key in CryptoDict:
  print("Tweets related to {}".format(key)+':')
  # Call the scraper function
  tweets_df = scraper(CryptoDict, key)
  # Cleaning tweets
  tweets_df['Tweets'] = tweets_df['Tweets'].apply(cleanTxt)
  # Do the sentiment analysis
  json = sentiment_analysis(tweets_df, key)
  #Upload to DB
  db = firestore.client()
  db.collection(u'sentiments').document(u'{}'.format(key)).set(json)
  #TO DO print when succesfully uploaded, else raise error
  #TO DO add a timestamp (or time interval for scraped tweets)

#Plot the polarity and subjectivity
plt.figure(figsize=(8,6))
for i in range(0, tweets_df.shape[0]):
  plt.scatter(tweets_df['Polarity'][i],tweets_df['Subjectivity'][i], color='Blue')

plt.title('Sentiment Analysis')
plt.xlabel('Polarity')
plt.ylabel('Subjectivity')
plt.show()

#Show the value counts
tweets_df['Analysis'].value_counts()

#Plot and visualize the counts
plt.title('Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')
tweets_df['Analysis'].value_counts().plot(kind='bar')
plt.show()